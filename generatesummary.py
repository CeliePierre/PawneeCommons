# -*- coding: utf-8 -*-
"""Generatesummary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w7sPbH4ZQGbBx8B_JTJ9UmR9ViIjQlww
"""

!pip install nltk

import pandas as pd
from transformers import AutoTokenizer, AutoModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from nltk.tokenize import sent_tokenize
import nltk

# Download the 'punkt' resource
nltk.download('punkt')

# Load the sentence-transformer model
model_name = "bert-base-uncased"  # Replace with the model you want to use
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Function to generate sentence embeddings
def generate_sentence_embeddings(text):
    sentences = sent_tokenize(text)
    sentence_embeddings = []
    for sentence in sentences:
        inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True)
        outputs = model(**inputs)
        sentence_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()
        sentence_embeddings.append(sentence_embedding)
    return sentence_embeddings

# Function to extract summaries from a CSV file
def extract_summary(csv_file):
    # Read CSV file into a DataFrame
    df = pd.read_csv(csv_file)

    # Combine dialogues into a single text
    dialogues = df['Line'].tolist()
    text = ' '.join(dialogues)

    # Generate sentence embeddings for the original text
    sentence_embeddings = generate_sentence_embeddings(text)

    # Perform clustering (optional)
    num_clusters = min(len(sentence_embeddings), 3)  # Adjust based on the number of sentences
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    kmeans.fit(sentence_embeddings)
    cluster_centers = kmeans.cluster_centers_

    # Extract representative sentences from each cluster for the summary
    representative_indices = pairwise_distances_argmin_min(cluster_centers, sentence_embeddings)[0]
    summary_sentences = [sent_tokenize(text)[i] for i in representative_indices]
    summary = " ".join(summary_sentences)

    return summary

# Process each CSV file and save summaries to a CSV file
csv_file_paths = [
                  's4e01.csv', 's4e02.csv', 's4e03.csv', 's4e04.csv', 's4e05.csv', 's4e06.csv',
                  's4e07.csv', 's4e08.csv', 's4e09.csv', 's4e10.csv', 's4e11.csv', 's4e12.csv',
                  's4e13.csv', 's4e14.csv', 's4e15.csv', 's4e16.csv', 's4e17.csv', 's4e18.csv',
                  's4e19.csv', 's4e20.csv', 's4e21.csv', 's4e22.csv',

                   's5e01.csv', 's4e02.csv', 's4e03.csv', 's5e04.csv', 's5e05.csv', 's5e06.csv',
                  's5e07.csv', 's5e08.csv', 's5e09.csv', 's5e10.csv', 's5e11.csv', 's5e12.csv',
                  's5e13.csv', 's5e14.csv', 's5e15.csv', 's5e16.csv', 's5e17.csv', 's5e18.csv',
                  's5e19.csv', 's5e20.csv', 's5e21.csv', 's5e22.csv',


                  's3e01.csv', 's3e02.csv', 's3e03.csv', 's3e04.csv', 's3e05.csv', 's3e06.csv',
                  's3e07.csv', 's3e08.csv', 's3e09.csv', 's3e10.csv', 's3e11.csv', 's3e12.csv',
                  's3e13.csv', 's3e14.csv', 's3e15.csv', 's3e16.csv']

summaries = []
for csv_file in csv_file_paths:
    summary = extract_summary(csv_file)
    summaries.append({'File': csv_file, 'Summary': summary})

# Save summaries to a CSV file
output_df = pd.DataFrame(summaries)
output_df.to_csv('summaries.csv', index=False)